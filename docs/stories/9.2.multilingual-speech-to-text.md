# Story 9.2: Multilingual Speech-to-Text with Deepgram & Swaram AI

**User Story:**
As a user,
I want to speak my queries in multiple languages and have them accurately converted to text,
So that the agent can understand me.

**Story Context:**
*   **Integrates with:** The PipeCat audio stream established in Story 1 and the `agent-service`.
*   **Technology:** Deepgram STT, Swaram AI, and the existing PipeCat integration.
*   **Follows pattern:** The existing microservices architecture and the event-driven nature of the PipeCat pipeline.
*   **Touch points:** This story adds new processors to the `agent-service`'s PipeCat pipeline to handle STT and language detection.

**Acceptance Criteria:**
1.  The `agent-service` pipeline processes the incoming audio stream from PipeCat with Deepgram STT.
2.  Swaram AI is used to detect the language of the user's speech.
3.  The Deepgram STT service is configured to use the language detected by Swaram AI for transcription.
4.  The transcribed text is accurately passed to the agent's core logic for processing.
5.  The system correctly transcribes speech in all supported languages (English, Hindi, Chhattisgarhi).
6.  The integration is covered by appropriate tests.
7.  No regression in existing functionality is verified.

**Technical Notes:**
*   **Integration Approach:** Add Deepgram STT and Swaram AI processors to the PipeCat pipeline within the `agent-service`. The Swaram AI processor will run first to detect the language, and its output will configure the Deepgram STT processor.
*   **Existing Pattern Reference:** This builds directly on the PipeCat pipeline pattern established in the previous story.
*   **Key Constraints:** The combined latency of language detection and transcription should be low enough to feel like a real-time conversation. API keys for Deepgram and Swaram AI must be managed securely.

**Minimal Risk Assessment:**

*   **Primary Risk:** The accuracy and latency of Deepgram STT and Swaram AI's language detection, especially for less common dialects or noisy environments.
*   **Mitigation:** Conduct extensive testing with diverse audio samples and accents in all supported languages. Implement fallback mechanisms (e.g., default to English if language detection fails).
*   **Rollback:** The STT and language detection components can be disabled within the PipeCat pipeline, allowing the system to revert to a text-only input or a simpler STT if issues arise.

**Compatibility Verification:**

*   [x] No breaking changes to existing APIs.
*   [x] Database changes (if any) are additive only. (No DB changes expected for this story).
*   [x] UI changes follow existing design patterns. (No direct UI changes for this story, but the transcribed text will be displayed).
*   [x] Performance impact is negligible.
